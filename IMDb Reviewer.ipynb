{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP ICA Project2.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdrydlTEeYw7"
      },
      "source": [
        "#Loading the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InDakrB_7_Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77cde50f-da2c-42b1-ae63-87f557be7420"
      },
      "source": [
        "! pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "! pip install pattern"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (90.0.4430.72-0ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "Requirement already satisfied: pattern in /usr/local/lib/python3.7/dist-packages (3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.7/dist-packages (from pattern) (18.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (from pattern) (20201018)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.7/dist-packages (from pattern) (1.0.7)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.2.5)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from pattern) (0.8.10)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.2.6)\n",
            "Requirement already satisfied: mysqlclient in /usr/local/lib/python3.7/dist-packages (from pattern) (2.0.3)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from pattern) (6.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.4.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.7.0)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.5.2)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (3.3.0)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (2.7.1)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (3.4.7)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (1.15.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser->pattern) (1.0.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (3.3.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->pattern) (3.2.1)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->pattern) (3.5.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.7/dist-packages (from portend>=2.1.1->cherrypy->pattern) (4.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (56.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->pattern) (1.14.5)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2018.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaF9Tc3f_9Vb"
      },
      "source": [
        "import selenium\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "import sys"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRGOmltO8FYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6629c826-a07e-452d-ffa0-3dceec7387b5"
      },
      "source": [
        "# Creating Driver Instance\n",
        "# install chromium, its driver, and selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (90.0.4430.72-0ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laxvihjBoYoX"
      },
      "source": [
        "# Loading the movies dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6fl81YhdqJb"
      },
      "source": [
        "def load_data():\n",
        "  import pandas as pd\n",
        "  global movie\n",
        "  global link\n",
        "  movie = pd.read_csv('/content/drive/MyDrive/MovieLens Dataset(1)/movie.csv')\n",
        "  link = pd.read_csv('/content/drive/MyDrive/MovieLens Dataset(1)/link.csv')\n",
        "  global movie_links\n",
        "  movie_links = pd.concat([movie,link],axis=1,join='inner')\n",
        "  movie_links.drop('tmdbId',axis=1,inplace=True)\n",
        "  df = movie_links['title'].str.split('(',expand=True)\n",
        "  df.rename({0:'title',1:'year'},axis=1,inplace=True)\n",
        "  df = df[['title','year']]\n",
        "  df['year'] = df['year'].str.replace(')','')\n",
        "  movie_links['year'] = df['year']\n",
        "  movie_links['title'] = df['title']\n",
        "  movie_links.index = movie_links['title']\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UAt6K-itaF9"
      },
      "source": [
        "#Scraping the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5184YGE3qDu"
      },
      "source": [
        "def extend_page(wd):\n",
        "  count = 20\n",
        "  from selenium.common.exceptions import NoSuchElementException,ElementNotInteractableException\n",
        "  prev_height = wd.execute_script('return document.body.scrollHeight')\n",
        "\n",
        "\n",
        "  while(count>0):\n",
        "    try:\n",
        "\n",
        "      wd.find_element_by_id(\"load-more-trigger\").click()\n",
        "      print(count)\n",
        "\n",
        "    except (ElementNotInteractableException,NoSuchElementException):\n",
        "      page = wd.page_source\n",
        "      return page\n",
        "    time.sleep(1)\n",
        "\n",
        "    wd.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
        "    new_height = wd.execute_script('return document.body.scrollHeight')\n",
        "\n",
        "    prev_height = new_height\n",
        "    count-=1\n",
        "  page = wd.page_source\n",
        "  return page\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S0kVp564H84"
      },
      "source": [
        "def scrape_reviews(page):\n",
        "  from bs4 import BeautifulSoup as bsoup\n",
        "  import requests\n",
        "  soup = bsoup(page,'lxml')\n",
        "  revs = soup.find_all('div',{'class':'text show-more__control'})\n",
        "  revs2 = soup.find_all('div',{'class':'text show-more__control clickable'})\n",
        "  rating = soup.find_all('div',{'class':'ipl-ratings-bar'})\n",
        "  ratings = []\n",
        "  global k\n",
        "  k = 0\n",
        "  reviews = []\n",
        "  for j,i in tqdm(enumerate(revs)):\n",
        "    reviews.append(i.text)\n",
        "    k+=1\n",
        "\n",
        "  for j,i in tqdm(enumerate(revs2)):\n",
        "    reviews.append(i.text)\n",
        "    k+=1\n",
        "\n",
        "  for j,i in tqdm(enumerate(rating)):\n",
        "    ratings.append(i.text.split('/')[0][-1])\n",
        "\n",
        "  ratings = [int(i) for i in ratings]\n",
        " \n",
        "  print(k,'reviews scraped')\n",
        "\n",
        "  return reviews,ratings"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ8PqH-m-jS8"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjmGlvmPlTSg"
      },
      "source": [
        "def preprocess(reviews):\n",
        "    import re\n",
        "    import gensim\n",
        "    from gensim.parsing.preprocessing import remove_stopwords\n",
        "    from gensim.models.phrases import Phrases,Phraser\n",
        "    from gensim.utils import lemmatize\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    cleaned_reviews = []\n",
        "    for rev in reviews:\n",
        "        text = re.sub('[^a-zA-Z]',' ',rev)\n",
        "        text = text.lower()\n",
        "        text = re.sub('(\\\\d|\\\\W)+',' ',text)\n",
        "        text = remove_stopwords(text)\n",
        "        cleaned_reviews.append(text.split())\n",
        "\n",
        "    lemmatized_reviews = []\n",
        "\n",
        "\n",
        "    for review in cleaned_reviews:\n",
        "        lemmatized_review = []\n",
        "        for word in review:\n",
        "            lemmatized_review.append(lemmatizer.lemmatize(word))\n",
        "        lemmatized_reviews.append(lemmatized_review)\n",
        "        \n",
        "\n",
        "    phrases = Phrases(lemmatized_reviews)\n",
        "    bigram = Phraser(phrases)\n",
        "\n",
        "    final_phrases = bigram[lemmatized_reviews]\n",
        "    \n",
        "    return final_phrases\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_7SGgcfsTNZ"
      },
      "source": [
        "# Data Labelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BknuDdgluvJ6"
      },
      "source": [
        "def labelling(reviews):\n",
        "  print('labelling')\n",
        "  import nltk\n",
        "  import pandas as pd\n",
        "  nltk.download('vader_lexicon')\n",
        "\n",
        "  from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "  sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "  labels = []\n",
        "\n",
        "  for review in reviews:\n",
        "\n",
        "    score = sid.polarity_scores(review)\n",
        "\n",
        "    if (score['compound'] >= 0.05 ):\n",
        "      labels.append(1)\n",
        "\n",
        "    elif (score['compound'] <= -0.05):\n",
        "      labels.append(2)\n",
        "\n",
        "    else:\n",
        "      labels.append(0)\n",
        "\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "  print('reviews:',len(reviews))\n",
        "  print('labels:',len(labels))\n",
        "\n",
        "\n",
        "  df['reviews'] = reviews\n",
        "  df['label'] = labels\n",
        "\n",
        "  return df\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCahr8vP-l83"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPVGzJL4oUZa"
      },
      "source": [
        "def gensim_embedding(all_phrases1):\n",
        "  print('embedding')\n",
        "  print('Shape of phrases',all_phrases1.shape)\n",
        "  import gensim\n",
        "  import numpy as np\n",
        "  from gensim.models import Word2Vec as wvec\n",
        "  from sklearn.decomposition import PCA\n",
        "\n",
        "  all_phrases1['cleaned_reviews2'] = all_phrases1['cleaned_reviews']\n",
        "\n",
        "  cbow = wvec(min_count=10,window=5,size=all_phrases1.shape[0],alpha=0.01,min_alpha=0.001,sample=6e-5,negative = 10)\n",
        "\n",
        "\n",
        "  dictionary = gensim.corpora.Dictionary(all_phrases1['cleaned_reviews2'])\n",
        "\n",
        "  model = wvec(min_count=10,window=5,size=all_phrases1.shape[0],alpha=0.01,min_alpha=0.001,sample=6e-5,negative = 10)\n",
        "\n",
        "  model.build_vocab(sentences=all_phrases1['cleaned_reviews2'])\n",
        "\n",
        "  model.train(all_phrases1,total_examples=cbow.corpus_count,epochs=200,)\n",
        "\n",
        "  vectors = model.wv.vectors\n",
        "\n",
        "  print(vectors.shape)\n",
        "\n",
        "  pca = PCA(n_components=100)\n",
        "\n",
        "  v2 = vectors.T\n",
        "  # v2 = vectors\n",
        "\n",
        "\n",
        "  print(v2.shape)\n",
        "\n",
        "\n",
        "  v2 = pca.fit_transform(v2) \n",
        "\n",
        "  return v2\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WVh-WuVBwzy"
      },
      "source": [
        "# Data Splitting and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCKXP0_vIfVF"
      },
      "source": [
        "def data_prep(phrases,vectors):\n",
        "  labels = phrases['label']\n",
        "  import keras\n",
        "  from keras.utils import to_categorical\n",
        "  import numpy as np\n",
        "\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  labels2 = to_categorical(labels,3)\n",
        "  vectors = vectors.reshape((vectors.shape[0],vectors.shape[1],1))\n",
        "  model = keras.models.load_model('/content/drive/MyDrive/NLP Project/lstm_classifier_4.0.h5')\n",
        "\n",
        "  pred = predict(model,vectors,labels2)\n",
        "\n",
        "  return pred"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eUXb00PW-I3"
      },
      "source": [
        "def predict(lstm,x_test,y_test):\n",
        "\n",
        "  from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "  from keras.utils import to_categorical\n",
        "\n",
        "  print('evaluating:')\n",
        "\n",
        "  lstm.evaluate(x_test,y_test)\n",
        "\n",
        "  revs = []\n",
        "\n",
        "  pred = lstm.predict(x_test)\n",
        "\n",
        "\n",
        "  print('shape of pred:',pred.shape)\n",
        "\n",
        "\n",
        "  print('0th index:',(pred[0]))\n",
        "\n",
        "  revs = []\n",
        "\n",
        "\n",
        "  for i in range(len(pred)):\n",
        "    max_ind = np.argmax(pred[i],axis=0)\n",
        "    revs.append(max_ind)\n",
        "    # print('revs length:',len(revs))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  revs = np.array(revs)\n",
        "\n",
        "  revs = to_categorical(revs,3)\n",
        "\n",
        "\n",
        "  print('Accuracy on testing data:',accuracy_score(revs,y_test))\n",
        "\n",
        "  print('\\n')\n",
        "\n",
        "  print(confusion_matrix(y_test.argmax(axis=1),revs.argmax(axis=1)))\n",
        "\n",
        "  revs = revs.argmax(axis=1)\n",
        "\n",
        "  return revs\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py1CGe1oB3xB"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwc-tsgUB6FV"
      },
      "source": [
        "def plot(pred,phrases,ratings,remove_words):\n",
        "\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  from os import path\n",
        "  from PIL import Image\n",
        "  from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "  neg = len(pred[pred==2])\n",
        "  pos = len(pred[pred==1])\n",
        "  neu = len(pred[pred==0])\n",
        "  groups = [pos,neg,neu]\n",
        "\n",
        "  labels = ['Positive','Negative','Neutral']\n",
        "  plt.rcParams['figure.figsize']=(17,10)\n",
        "  fig,axs = plt.subplots(2,2)\n",
        "\n",
        "  axs[0,0].hist(ratings,bins=np.arange(11)-0.5,rwidth=0.5)\n",
        "  axs[0,0].set_xticks(range(10),)\n",
        "  axs[0,0].set_xticklabels([1,2,3,4,5,6,7,8,9,10])\n",
        "\n",
        "  axs[0,1].pie(groups,labels=labels,autopct='%.2f')\n",
        "\n",
        "  gensim_stopwords = STOPWORDS.union(set(remove_words))\n",
        "\n",
        "\n",
        "  all_text = [text for i in phrases['reviews'] for text in i]\n",
        "  all_text = ''.join(phrases['reviews'][:])\n",
        "  # all_text\n",
        "\n",
        "  filtered_text = [word for word in all_text if word not in gensim_stopwords]\n",
        "  wc = WordCloud().generate(filtered_text)\n",
        "\n",
        "  axs[1,0].imshow(wc)\n",
        "  axs[1,0].set_xticks([])\n",
        "  axs[1,0].set_yticks([])\n",
        "  axs[1,0].set_title('High frequency words present in the scraped reviews')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BHdmLp_SzER"
      },
      "source": [
        "def html_plot(pred,phrases,ratings,remove_words):\n",
        "\n",
        "  !pip install mpld3\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  from os import path\n",
        "  from PIL import Image\n",
        "  import gensim\n",
        "  from gensim.parsing.preprocessing import STOPWORDS\n",
        "  from nltk.corpus import stopwords\n",
        "  from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\n",
        "  import matplotlib.pyplot as plt\n",
        "  import mpld3\n",
        "\n",
        "\n",
        "  neg = len(pred[pred==2])\n",
        "  pos = len(pred[pred==1])\n",
        "  neu = len(pred[pred==0])\n",
        "  groups = [pos,neg,neu]\n",
        "\n",
        "  labels = ['Positive','Negative','Neutral']\n",
        "\n",
        "  # fig = plt.figure()\n",
        "  plt.rcParams['figure.figsize']=(17,10)\n",
        "  fig,axs = plt.subplots(2,2)\n",
        "\n",
        "\n",
        "  axs[0,0].hist(ratings,bins=np.arange(11)-0.5,rwidth=0.5)\n",
        "  axs[0,0].set_xticks(range(10),)\n",
        "  axs[0,0].set_xticklabels([1,2,3,4,5,6,7,8,9,10])\n",
        "  axs[0,0].set_xlabel('Ratings')\n",
        "  axs[0,0].set_ylabel('Number of Users')\n",
        "  axs[0,0].set_title('Frequency Distribution of Ratings',fontsize=15)\n",
        "\n",
        "\n",
        "  axs[0,1].pie(groups,labels=labels,autopct='%.2f')\n",
        "  axs[0,1].set_title('Percentage of positive, negative and neutral reviews',fontsize=15)\n",
        "  axs[0,1].xaxis.set_visible(False)\n",
        "  axs[0,1].yaxis.set_visible(False)\n",
        "  nltk_stopwords = stopwords.words(\"english\")\n",
        "\n",
        "  nltk_stopwords.extend(remove_words)\n",
        "\n",
        "\n",
        "  all_text = [text for i in phrases['reviews'] for text in i]\n",
        "  all_text = ' '.join(phrases['reviews'][:])\n",
        "  print('removing stopwords...')\n",
        "  gensim_stopwords = STOPWORDS.union(set(remove_words))\n",
        "  # all_text\n",
        "  all_text_list = all_text.split()\n",
        "  filtered_text_list = [word for word in all_text_list if word not in nltk_stopwords]\n",
        "\n",
        "  # print('diff between all_text and filtered_text_list:',len(all_text_list)==len(filtered_text_list))\n",
        "\n",
        "  filtered_text = ' '.join(filtered_text_list)\n",
        "\n",
        "  # print('filtered_text:',filtered_text)\n",
        "  # print('all_text:',all_text_list[:10])\n",
        "\n",
        "\n",
        "  wc = WordCloud().generate(all_text)\n",
        "  wc = WordCloud().generate(filtered_text)\n",
        "\n",
        "\n",
        "\n",
        "  axs[1,0].imshow(wc)\n",
        "  axs[1,0].set_xticks([])\n",
        "  axs[1,0].set_yticks([])\n",
        "  axs[1,0].set_title('High frequency words present in the scraped reviews',fontsize=15)\n",
        "\n",
        "\n",
        "  # axs[1,1].set_xticks([])\n",
        "  # axs[1,1].set_yticks([])\n",
        "  axs[1,1].xaxis.set_visible(False)\n",
        "  axs[1,1].yaxis.set_visible(False)\n",
        "\n",
        "\n",
        "  plt.savefig(\"plot.png\")\n",
        "\n",
        "  html_plot2 = mpld3.fig_to_html(fig)\n",
        "  c = '''<!doctype html>\n",
        "  <html>\n",
        "  <head>\n",
        "  </head>\n",
        "  <body style=\"background-image: linear-gradient(blue,darkblue); height: 700px;\">\n",
        "\n",
        "  <img src = \"{{url_for('static',filename='/content/plot.png')}}\"\n",
        "  width='500'\n",
        "  height='500'\n",
        "  style=\"padding-top:150px;padding-left:500px\"\n",
        "  >\n",
        "\n",
        "  </body>\n",
        "  </html>'''\n",
        "\n",
        "\n",
        "  html_file2 = open(\"/content/templates/plot.html\",\"w\") # Saving the html file\n",
        "  html_file2.write(html_plot2) # Writing the file\n",
        "  html_file2.close() # Closing the file\n",
        "\n",
        "  return html_file2\n",
        "  # return 0\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eODQnteVSrQ"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC5bNGPyVSnu"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teucKi9EVSl0"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNnIBRI5VSju"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsII-CzmlT9N"
      },
      "source": [
        "def scrape_stopwords(wd,id):\n",
        "  wd.get(\"https://www.imdb.com/title/{}/fullcredits?ref_=tt_ql_1\".format(id))\n",
        "\n",
        "  count = 10\n",
        "  rev1 = []\n",
        "  prev_height = wd.execute_script('return document.body.scrollHeight')\n",
        "\n",
        "  while(count>0):\n",
        "    wd.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
        "    time.sleep(1)\n",
        "    new_height = wd.execute_script('return document.body.scrollHeight')\n",
        "    count-=1\n",
        "  page = wd.page_source\n",
        "\n",
        "  from bs4 import BeautifulSoup as bsoup\n",
        "  from tqdm import tqdm\n",
        "  import requests\n",
        "  soup = bsoup(page,'lxml')\n",
        "  revs = soup.find_all('a')\n",
        "  remove_words = []\n",
        "  for j,i in tqdm(enumerate(revs)):\n",
        "    remove_words.append(i.text)\n",
        "\n",
        "  remove_words2 = []\n",
        "  for j in remove_words:\n",
        "\n",
        "    if(j=='\\n'):\n",
        "      continue\n",
        "\n",
        "    if(' ' in j):\n",
        "\n",
        "      remove_words2.extend(j.split())\n",
        "\n",
        "    else:\n",
        "      remove_words2.append(j)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(remove_words2)\n",
        "\n",
        "  return remove_words2\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwJvKMXe_BId"
      },
      "source": [
        "# Driver function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhgJ0HSoPfN4"
      },
      "source": [
        "def get_movie(selection):\n",
        "  load_data()\n",
        "  movie = selection\n",
        "  movie = movie.lower()\n",
        "  info = movie.title()\n",
        "  info = info+' '\n",
        "  print(info)\n",
        "\n",
        "  if (info not in movie_links['title']):\n",
        "\n",
        "    if (info[:3]=='The'):\n",
        "      l1 = info.split()\n",
        "\n",
        "      del l1[0]\n",
        "\n",
        "      info = ' '.join(l1)\n",
        "      info+=', The '\n",
        "      print(info)\n",
        "      if (info not in movie_links['title']):\n",
        "        print('Movie not recognized')\n",
        "        return 0\n",
        "\n",
        "    else:\n",
        "        print('Movie not recognized')\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "  id = str(movie_links.imdbId.loc[info])\n",
        "  id = 'tt0'+id\n",
        "  wd = webdriver.Chrome('chromedriver',options=options)\n",
        "\n",
        "  wd.get('https://www.imdb.com/title/{}/reviews?ref_=tt_urv'.format(id))\n",
        "\n",
        "  print('Extending the page...')\n",
        "  \n",
        "  page = extend_page(wd)\n",
        "\n",
        "  reviews,ratings = scrape_reviews(page)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  print(len(reviews))\n",
        "\n",
        "  if (len(reviews)>=100):\n",
        "    print('Labelling the reviews...')\n",
        "    phrases = labelling(reviews) \n",
        "\n",
        "    print('preprocessing the reviews...')\n",
        "    phrases['cleaned_reviews'] = preprocess(phrases['reviews'])\n",
        "\n",
        "    print('Embedding...')\n",
        "\n",
        "    vectors = gensim_embedding(phrases)\n",
        "\n",
        "\n",
        "    print('Data collection and preprocessing complete.')\n",
        "\n",
        "\n",
        "    pred = data_prep(phrases,vectors)\n",
        "\n",
        "    remove_words = scrape_stopwords(wd,id)\n",
        "\n",
        "    remove_words.append(selection)\n",
        "    remove_words.append(movie)\n",
        "    remove_words.append(movie.title())\n",
        "\n",
        "    remove_words = list(set(remove_words))\n",
        "\n",
        "\n",
        "\n",
        "    plot_file = html_plot(pred,phrases,ratings,remove_words)\n",
        "\n",
        "    return plot_file\n",
        "\n",
        "  else:\n",
        "    # mnr = '''\n",
        "    # <!DOCTYPE html>\n",
        "    # <html>\n",
        "    #     <head style=\"text-align: center;\">Movie Not Recognized</head>\n",
        "    #     <body>\n",
        "    #         <h1 style=\"text-align: center;\">Movie Not Recognized</h1>\n",
        "    #     </body>\n",
        "    # </html>'''\n",
        "\n",
        "    # html_file3 = open(\"content/templates/mnr.html\",\"w\")\n",
        "    # html_file3.write(mnr)\n",
        "    # html_file3.close()\n",
        "    # return html_file3\n",
        "    str2 = \"Not enough reviews\"\n",
        "    return str2\n",
        "  \n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DldWFU3d1r_N"
      },
      "source": [
        "# Deployment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9swcJmwt48Rg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b784e6f-e463-476a-a55b-fa207769004b"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.7/dist-packages (0.0.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcgPLHod4zow"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duKNNuEkP5GD"
      },
      "source": [
        "def load_mymodel():\n",
        "    import keras\n",
        "    global mymodel\n",
        "    mymodel = keras.models.load_model('/content/drive/MyDrive/NLP Project/imdb_classifier_4.0.h5')\n",
        "    return mymodel\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukBXKE-Vgnna"
      },
      "source": [
        "home = '''<!doctype html>\n",
        "<html lang=\"en\">\n",
        "\n",
        "<title>IMDb Reviewer</title>\n",
        "\n",
        "<head>\n",
        "    <title>Imdb Reviewer</title>\n",
        "</head>\n",
        "\n",
        "<body style=\"background-image: linear-gradient(blue,darkblue); height: 750px;\">\n",
        "    <h1 style=\"text-align:center;color:black;padding-top: 200px;\" >IMDb Reviewer</h1>\n",
        "    <div class=\"jumbotron text-center\"></div>\n",
        "    <form style=\"text-align:center;\" action=\"{{url_for('load_movie')}}\" method=\"post\">\n",
        "    <label for=\"movie\" style=\"font-size:25px;\">Enter the movie whose reviews you would like to see:</label>\n",
        "    <input align=\"center\" type=\"text\" style=\"background-color:black; font-size: 20px;color:white;\" id=\"movie\" name=\"film\" placeholder=\"Search\">\n",
        "    <button type=\"submit\" style=\"background-color:darkblue;color:white;font-size:20px;\">Enter</button>\n",
        "  </form>\n",
        " </div\n",
        "</body>\n",
        "\n",
        "</html>'''\n",
        "\n",
        "\n",
        "ner = '''<!DOCTYPE html>\n",
        "<html>\n",
        "    <head>\n",
        "        <title>Not enough reviews to scrape for this movie</title>\n",
        "    </head>\n",
        "\n",
        "    <body style= \"background-image: linear-gradient(blue,darkblue); height: 750px;\">\n",
        "        <h1 style=\"text-align: center;color:black;padding-top:200px;\">There are not enough reviews given for this movie. Please try again later.</h1>\n",
        "    </body>\n",
        "</html>'''\n",
        "\n",
        "\n",
        "mnr = '''<!DOCTYPE html>\n",
        "<html>\n",
        "    <head style=\"text-align: center;\"></head>\n",
        "    <body style= \"background-image: linear-gradient(blue,darkblue); height: 750px;\">\n",
        "        <h1 style=\"text-align: center;\">Movie Not Recognized</h1>\n",
        "    </body>\n",
        "</html>'''"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZTywao2gskE"
      },
      "source": [
        "!mkdir '/content/templates'\n",
        "\n",
        "index_file = open('/content/templates/index.html',\"w\")\n",
        "index_file.write(home)\n",
        "index_file.close()\n",
        "\n",
        "ner_file = open('/content/templates/ner.html','w')\n",
        "ner_file.write(ner)\n",
        "ner_file.close()\n",
        "\n",
        "\n",
        "mnr_file = open('/content/templates/mnr.html','w')\n",
        "mnr_file.write(mnr)\n",
        "mnr_file.close()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCy8CRvjP5EG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215e9b68-231b-4886-d782-d2c434c38e9a"
      },
      "source": [
        "from flask import Flask,request,render_template\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "  return render_template(\"index.html\")\n",
        "\n",
        "@app.route('/',methods=[\"GET\",\"POST\"])\n",
        "\n",
        "def load_movie():\n",
        "  if request.method==\"POST\":\n",
        "    movie = request.form.get(\"film\")\n",
        "\n",
        "    plot_file = get_movie(movie)\n",
        "\n",
        "    print(type(plot_file))\n",
        "\n",
        "    if(type(plot_file)!=str and plot_file!=0):\n",
        "      return render_template(\"plot.html\")\n",
        "\n",
        "    if(plot_file==0):\n",
        "      return render_template(\"mnr.html\")\n",
        "      pass\n",
        "\n",
        "    if(type(plot_file)==str):\n",
        "      return render_template(\"ner.html\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "  app.run()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://6bad1b0eb076.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [08/May/2021 11:50:30] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [08/May/2021 11:50:31] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [08/May/2021 11:50:34] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Khgfd \n",
            "Movie not recognized\n",
            "<class 'int'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [08/May/2021 11:50:43] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Blackthorn \n",
            "Extending the page...\n",
            "20\n",
            "19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "39it [00:00, 23156.55it/s]\n",
            "19it [00:00, 15583.06it/s]\n",
            "49it [00:00, 43349.69it/s]\n",
            "127.0.0.1 - - [08/May/2021 11:50:54] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "58 reviews scraped\n",
            "58\n",
            "<class 'str'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [08/May/2021 11:51:00] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkxhqF6rBYQG"
      },
      "source": [
        ""
      ],
      "execution_count": 52,
      "outputs": []
    }
  ]
}